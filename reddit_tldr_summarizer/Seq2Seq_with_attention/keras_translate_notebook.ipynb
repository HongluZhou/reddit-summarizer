{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing all the things we'll need.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, CuDNNLSTM, Flatten, TimeDistributed, Dropout, LSTMCell, RNN\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# This enables the Jupyter backend on some matplotlib installations.\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "# Turn off interactive plots. iplt doesn't work well with Jupyter.\n",
    "plt.ioff()\n",
    "\n",
    "import unicodedata\n",
    "import pymongo\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import math\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "len_input = 300\n",
    "len_output = 30\n",
    "vocab_in_size = 2475\n",
    "vocab_out_size = 2475"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'lstm_with_attention/transpose_1:0' shape=(?, 300, ?) dtype=float32>,\n",
       " <tf.Tensor 'lstm_with_attention/while/Exit_3:0' shape=(?, 512) dtype=float32>,\n",
       " <tf.Tensor 'lstm_with_attention/while/Exit_4:0' shape=(?, 512) dtype=float32>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RNN \"Cell\" classes in Keras perform the actual data transformations at each timestep. Therefore, in order\n",
    "# to add attention to LSTM, we need to make a custom subclass of LSTMCell.\n",
    "class AttentionLSTMCell(LSTMCell):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.attentionMode = False\n",
    "        super(AttentionLSTMCell, self).__init__(**kwargs)\n",
    "    \n",
    "    # Build is called to initialize the variables that our cell will use. We will let other Keras\n",
    "    # classes (e.g. \"Dense\") actually initialize these variables.\n",
    "    @tf_utils.shape_type_conversion\n",
    "    def build(self, input_shape):        \n",
    "        # Converts the input sequence into a sequence which can be matched up to the internal\n",
    "        # hidden state.\n",
    "        self.dense_constant = TimeDistributed(Dense(self.units, name=\"AttLstmInternal_DenseConstant\"))\n",
    "        \n",
    "        # Transforms the internal hidden state into something that can be used by the attention\n",
    "        # mechanism.\n",
    "        self.dense_state = Dense(self.units, name=\"AttLstmInternal_DenseState\")\n",
    "        \n",
    "        # Transforms the combined hidden state and converted input sequence into a vector of\n",
    "        # probabilities for attention.\n",
    "        self.dense_transform = Dense(1, name=\"AttLstmInternal_DenseTransform\")\n",
    "        \n",
    "        # We will augment the input into LSTMCell by concatenating the context vector. Modify\n",
    "        # input_shape to reflect this.\n",
    "        batch, input_dim = input_shape[0]\n",
    "        batch, timesteps, context_size = input_shape[-1]\n",
    "        lstm_input = (batch, input_dim + context_size)\n",
    "        \n",
    "        # The LSTMCell superclass expects no constant input, so strip that out.\n",
    "        return super(AttentionLSTMCell, self).build(lstm_input)\n",
    "    \n",
    "    # This must be called before call(). The \"input sequence\" is the output from the \n",
    "    # encoder. This function will do some pre-processing on that sequence which will\n",
    "    # then be used in subsequent calls.\n",
    "    def setInputSequence(self, input_seq):\n",
    "        self.input_seq = input_seq\n",
    "        self.input_seq_shaped = self.dense_constant(input_seq)\n",
    "        self.timesteps = tf.shape(self.input_seq)[-2]\n",
    "    \n",
    "    # This is a utility method to adjust the output of this cell. When attention mode is\n",
    "    # turned on, the cell outputs attention probability vectors across the input sequence.\n",
    "    def setAttentionMode(self, mode_on=False):\n",
    "        self.attentionMode = mode_on\n",
    "    \n",
    "    # This method sets up the computational graph for the cell. It implements the actual logic\n",
    "    # that the model follows.\n",
    "    def call(self, inputs, states, constants):\n",
    "        # Separate the state list into the two discrete state vectors.\n",
    "        # ytm is the \"memory state\", stm is the \"carry state\".\n",
    "        ytm, stm = states\n",
    "        # We will use the \"carry state\" to guide the attention mechanism. Repeat it across all\n",
    "        # input timesteps to perform some calculations on it.\n",
    "        stm_repeated = K.repeat(self.dense_state(stm), self.timesteps)\n",
    "        # Now apply our \"dense_transform\" operation on the sum of our transformed \"carry state\" \n",
    "        # and all encoder states. This will squash the resultant sum down to a vector of size\n",
    "        # [batch,timesteps,1]\n",
    "        # Note: Most sources I encounter use tanh for the activation here. I have found with this dataset\n",
    "        # and this model, relu seems to perform better. It makes the attention mechanism far more crisp\n",
    "        # and produces better translation performance, especially with respect to proper sentence termination.\n",
    "        combined_stm_input = self.dense_transform(\n",
    "            keras.activations.relu(stm_repeated + self.input_seq_shaped))\n",
    "        # Performing a softmax generates a log probability for each encoder output to receive attention.\n",
    "        score_vector = keras.activations.softmax(combined_stm_input, 1)\n",
    "        # In this implementation, we grant \"partial attention\" to each encoder output based on \n",
    "        # it's log probability accumulated above. Other options would be to only give attention\n",
    "        # to the highest probability encoder output or some similar set.\n",
    "        context_vector = K.sum(score_vector * self.input_seq, 1)\n",
    "        \n",
    "        # Finally, mutate the input vector. It will now contain the traditional inputs (like the seq2seq\n",
    "        # we trained above) in addition to the attention context vector we calculated earlier in this method.\n",
    "        inputs = K.concatenate([inputs, context_vector])\n",
    "        \n",
    "        # Call into the super-class to invoke the LSTM math.\n",
    "        res = super(AttentionLSTMCell, self).call(inputs=inputs, states=states)\n",
    "        \n",
    "        # This if statement switches the return value of this method if \"attentionMode\" is turned on.\n",
    "        if(self.attentionMode):\n",
    "            return (K.reshape(score_vector, (-1, self.timesteps)), res[1])\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "# Custom implementation of the Keras LSTM that adds an attention mechanism.\n",
    "# This is implemented by taking an additional input (using the \"constants\" of the\n",
    "# RNN class) into the LSTM: The encoder output vectors across the entire input sequence.\n",
    "class LSTMWithAttention(RNN):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        cell = AttentionLSTMCell(units=units)\n",
    "        self.units = units\n",
    "        super(LSTMWithAttention, self).__init__(cell, **kwargs)\n",
    "        \n",
    "    @tf_utils.shape_type_conversion\n",
    "    def build(self, input_shape):\n",
    "        self.input_dim = input_shape[0][-1]\n",
    "        self.timesteps = input_shape[0][-2]\n",
    "        return super(LSTMWithAttention, self).build(input_shape) \n",
    "    \n",
    "    # This call is invoked with the entire time sequence. The RNN sub-class is responsible\n",
    "    # for breaking this up into calls into the cell for each step.\n",
    "    # The \"constants\" variable is the key to our implementation. It was specifically added\n",
    "    # to Keras to accomodate the \"attention\" mechanism we are implementing.\n",
    "    def call(self, x, constants, **kwargs):\n",
    "        if isinstance(x, list):\n",
    "            self.x_initial = x[0]\n",
    "        else:\n",
    "            self.x_initial = x\n",
    "        \n",
    "        # The only difference in the LSTM computational graph really comes from the custom\n",
    "        # LSTM Cell that we utilize.\n",
    "        self.cell._dropout_mask = None\n",
    "        self.cell._recurrent_dropout_mask = None\n",
    "        self.cell.setInputSequence(constants[0])\n",
    "        return super(LSTMWithAttention, self).call(inputs=x, constants=constants, **kwargs)\n",
    "\n",
    "# Below is test code to validate that this LSTM class and the associated cell create a\n",
    "# valid computational graph.\n",
    "test = LSTMWithAttention(units=units, return_sequences=True, return_state=True)\n",
    "test.cell.setAttentionMode(True)\n",
    "attenc_inputs2 = Input(shape=(len_input,))\n",
    "attenc_emb2 = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n",
    "test(inputs=attenc_emb2(attenc_inputs2), constants=attenc_emb2(attenc_inputs2), initial_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Re-create an entirely new model and set of layers for the attention model\n",
    "\n",
    "# Encoder Layers\n",
    "attenc_inputs = Input(shape=(len_input,), name=\"attenc_inputs\")\n",
    "attenc_emb = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n",
    "attenc_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n",
    "attenc_outputs, attstate_h, attstate_c = attenc_lstm(attenc_emb(attenc_inputs))\n",
    "attenc_states = [attstate_h, attstate_c]\n",
    "\n",
    "attdec_inputs = Input(shape=(None,))\n",
    "attdec_emb = Embedding(input_dim=vocab_out_size, output_dim=embedding_dim)\n",
    "attdec_lstm = LSTMWithAttention(units=units, return_sequences=True, return_state=True)\n",
    "# Note that the only real difference here is that we are feeding attenc_outputs to the decoder now.\n",
    "# Nice and clean!\n",
    "attdec_lstm_out, _, _ = attdec_lstm(inputs=attdec_emb(attdec_inputs), \n",
    "                                    constants=attenc_outputs, \n",
    "                                    initial_state=attenc_states)\n",
    "attdec_d1 = Dense(units, activation=\"relu\")\n",
    "attdec_d2 = Dense(vocab_out_size, activation=\"softmax\")\n",
    "attdec_out = attdec_d2(Dropout(rate=.4)(attdec_d1(Dropout(rate=.4)(attdec_lstm_out))))\n",
    "\n",
    "attmodel = Model([attenc_inputs, attdec_inputs], attdec_out)\n",
    "attmodel.compile(optimizer=tf.keras.optimizers.Adam(), loss=\"sparse_categorical_crossentropy\", metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLDRSequence(Sequence):\n",
    "\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        client = pymongo.MongoClient()\n",
    "        self.tldr = client['test']['tldr_clean']\n",
    "        self.thread = threading.Thread(target=self.prepare_data)\n",
    "        self.thread.start()\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        cursor = self.tldr.aggregate([{'$sample':{'size': self.batch_size}}])\n",
    "        self.input_mtx = np.zeros((self.batch_size, len_input))\n",
    "        self.input_mtx[:, 0] = 2\n",
    "        self.teacher_mtx = np.zeros((self.batch_size, len_output))\n",
    "        self.teacher_mtx[:, 0] = 2\n",
    "        self.output_mtx = np.zeros((self.batch_size, len_output))\n",
    "        \n",
    "        for idx, data in enumerate(cursor):\n",
    "            vo = data['summary_vec_must_word']\n",
    "            lo = len(vo)\n",
    "            vi = data['content_vec']\n",
    "            li = data['content_len']\n",
    "            \n",
    "            self.input_mtx[idx, 1: 1+li] = vi\n",
    "            self.input_mtx[idx, 1+li] = 3\n",
    "            self.teacher_mtx[idx, 1: 1+lo] = vo\n",
    "            self.teacher_mtx[idx, 1+lo] = 3\n",
    "            self.output_mtx[idx, 0:lo] = vo\n",
    "            self.output_mtx[idx, lo] = 3\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return 500\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        self.thread.join()\n",
    "        input_mtx = self.input_mtx.copy()\n",
    "        teacher_mtx = self.teacher_mtx.copy()\n",
    "        output_mtx = self.output_mtx.copy()\n",
    "        \n",
    "        self.thread = threading.Thread(target=self.prepare_data)    \n",
    "        self.thread.start()\n",
    "        return [input_mtx, teacher_mtx], output_mtx[:,:,None]\n",
    "    \n",
    "Gen = TLDRSequence(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/200\n",
      " 40/500 [=>............................] - ETA: 3:09 - loss: 2.1585 - sparse_categorical_accuracy: 0.6313"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "atthist = attmodel.fit_generator(Gen, epochs=epochs, initial_epoch=55)\n",
    "\n",
    "# Plot the results of the training.\n",
    "plt.plot(atthist.history['sparse_categorical_accuracy'], label=\"Training loss\")\n",
    "plt.plot(atthist.history['val_sparse_categorical_accuracy'], label=\"Validation loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "attmodel.save_weights(\"attention_trained_weights.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference With Attention\n",
    "Now that we've got an attention model, let's test it, similar to above. The inference models don't change much from our seq2seq implementation, again with the exception of feeding in the encoder outputs to the decoder (which we already did in train() above).\n",
    "\n",
    "One thing I noticed about this attention model is that it actually seems to perform worse on completely novel data (e.g. phrases that aren't at all related to the ones in the training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageIndex():\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = {}\n",
    "        \n",
    "    def create_index(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = sorted(\n",
    "            w\n",
    "            for w, num in self.vocab.items()\n",
    "            if num > 18500\n",
    "        )\n",
    "        self.word2idx[\"<pad>\"] = 0\n",
    "        self.idx2word[0] = \"<pad>\"\n",
    "        self.word2idx[\"<noword>\"] = 1\n",
    "        self.idx2word[1] = \"<noword>\"\n",
    "        self.word2idx[\"<start>\"] = 2\n",
    "        self.idx2word[2] = \"<start>\"\n",
    "        self.word2idx[\"<end>\"] = 3\n",
    "        self.idx2word[3] = \"<end>\"\n",
    "        \n",
    "        for i,word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = i + 4\n",
    "            self.idx2word[i+4] = word\n",
    "            \n",
    "    def update_vocab(self, text):\n",
    "        self.vocab.update(text.split())\n",
    "        \n",
    "LangIdx = LanguageIndex()\n",
    "\n",
    "import json\n",
    "with open('vocab.txt', 'r') as f:\n",
    "    LangIdx.vocab = json.load(f)\n",
    "\n",
    "LangIdx.create_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_vector(sentence, lang):\n",
    "    pre = \"<start> \" + sentence.lower() + \" <end>\"\n",
    "    vec = np.zeros(len_input)\n",
    "    sentence_list = [lang.word2idx.get(s, 1) for s in pre.split(' ')]\n",
    "    for i,w in enumerate(sentence_list):\n",
    "        vec[i] = w\n",
    "    return vec\n",
    "\n",
    "# Given an input string, an encoder model (infenc_model) and a decoder model (infmodel),\n",
    "# return a translated string.\n",
    "def translate(input_sentence, infenc_model, infmodel, attention=True):\n",
    "    sv = sentence_to_vector(input_sentence, LangIdx)\n",
    "    # Reshape so we can use the encoder model. New shape=[samples,sequence length]\n",
    "    sv = sv.reshape(1,len(sv))\n",
    "    [emb_out, sh, sc] = infenc_model.predict(x=sv)\n",
    "    \n",
    "    i = 0\n",
    "    start_vec = LangIdx.word2idx[\"<start>\"]\n",
    "    stop_vec = LangIdx.word2idx[\"<end>\"]\n",
    "    # We will continuously feed cur_vec as an input into the decoder to produce the next word,\n",
    "    # which will be assigned to cur_vec. Start it with \"<start>\".\n",
    "    cur_vec = np.zeros((1,1))\n",
    "    cur_vec[0,0] = start_vec\n",
    "    cur_word = \"<start>\"\n",
    "    output_sentence = \"\"\n",
    "    # Start doing the feeding. Terminate when the model predicts an \"<end>\" or we reach the end\n",
    "    # of the max target language sentence length.\n",
    "    while cur_word != \"<end>\" and i < (len_output-1):\n",
    "        i += 1\n",
    "        if cur_word != \"<start>\":\n",
    "            output_sentence = output_sentence + \" \" + cur_word\n",
    "        x_in = [cur_vec, sh, sc]\n",
    "        # This will allow us to accomodate attention models, which we will talk about later.\n",
    "        if attention:\n",
    "            x_in += [emb_out]\n",
    "        [nvec, sh, sc] = infmodel.predict(x=x_in)\n",
    "        # The output of the model is a massive softmax vector with one spot for every possible word. Convert\n",
    "        # it to a word ID using argmax().\n",
    "        supresswords = [*map(LangIdx.word2idx.get, \n",
    "                             ['<noword>', 'i', 'you', 'good', 'dick', 'is', 'was', 'has', 's']\n",
    "                            )]\n",
    "        nvec[0,0,supresswords] = 0\n",
    "        cur_vec[0,0] = np.argmax(nvec[0,0])\n",
    "        cur_word = LangIdx.idx2word[np.argmax(nvec[0,0])]\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i m a dick\n"
     ]
    }
   ],
   "source": [
    "def createAttentionInference(attention_mode=False):\n",
    "    # Create an inference model using the layers already trained above.\n",
    "    attencoder_model = Model(attenc_inputs, [attenc_outputs, attstate_h, attstate_c])\n",
    "    state_input_h = Input(shape=(units,), name=\"state_input_h\")\n",
    "    state_input_c = Input(shape=(units,), name=\"state_input_c\")\n",
    "    attenc_seq_out = Input(shape=attenc_outputs.get_shape()[1:], name=\"attenc_seq_out\")\n",
    "    inf_attdec_inputs = Input(shape=(None,), name=\"inf_attdec_inputs\")\n",
    "    attdec_lstm.cell.setAttentionMode(attention_mode)\n",
    "    attdec_res, attdec_h, attdec_c = attdec_lstm(attdec_emb(inf_attdec_inputs), \n",
    "                                                 initial_state=[state_input_h, state_input_c], \n",
    "                                                 constants=attenc_seq_out)\n",
    "    attinf_model = None\n",
    "    if not attention_mode:\n",
    "        inf_attdec_out = attdec_d2(attdec_d1(attdec_res))\n",
    "        attinf_model = Model(inputs=[inf_attdec_inputs, state_input_h, state_input_c, attenc_seq_out], \n",
    "                             outputs=[inf_attdec_out, attdec_h, attdec_c])\n",
    "    else:\n",
    "        attinf_model = Model(inputs=[inf_attdec_inputs, state_input_h, state_input_c, attenc_seq_out], \n",
    "                             outputs=[attdec_res, attdec_h, attdec_c])\n",
    "    return attencoder_model, attinf_model\n",
    "\n",
    "attencoder_model, attinf_model = createAttentionInference()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(translate(\"i really need to put together a respect thread here s a woefully inaccurate summary kefka palazzo is the final boss of final fantasy in the first half of the game he is the court mage under the evil conqueror emperor gestahl he becomes gestahl s top general leading troops to conquer nation after nation with crazy powerful magic science magitek when conquering nashe he grabs one of your crew who s powered by magitek and uses her as a puppet to genocide the town when conquering doma he gets sick of waiting and poisons their water supply eradicating everyone then he lols and heads home this actually pisses off his boss gestahl who imprisons him kefka lols his way out of prison to conquer thamasa where he murders their spirit gods espers he uses the power he gains to make a giant floating island and give it to gestahl they re buddies again your crew shows up gestahl nopes you all into ice kefka uses this opportunity to backstab gestahl he lols and tosses his boss and all your crew off his floating island some of you are caught by your team s airship some get dead like storyline death the really real actual no coming back total splat factor dead kefka lols and reks the fucking world a year passes your crew is all shitfucked and scattered across the planet kefka is lord of the god kings he pretty much just sits on his skyscraper tower lolling gloating and slaughtering people by the millions for not worshipping him\", attencoder_model, attinf_model, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you re not a good idea\n"
     ]
    }
   ],
   "source": [
    "print(translate(\n",
    "    \"not op and only a noob accountant but i helped a fledgling hostel with the financial section of it s business plan these guys were friends brothers who just simply pooled their money together and jumped into the hostel business because they saw an opportunity for profit if they ll ever reach profit is debatable number one expense was the labor used to construct the bathroom shower area for the guests if you take one thing away from this it s don t do what they did hire illegal unlicensed labor to do plumbing these guys paid out the ass for the initial labor and now have to pay even more because the job was done badly and water was getting under the tiles and rotting away the wood underneath as for bedding they just made a bulk order from ikea for all the bunk beds sheets washing was their other large expense they organized a contract with a laundromat to take advantage of bulk discounts most important thing location admittedly these guys had a bad location and that fact will show up in the reviews from customers they use websites such as hostelworld to fill their rooms another expense that shouldn t be skimped out on is decoration depending on the type of customer you wish to attract you need to decorate accordingly whether it s artsy hand painted illustrations cool stencils or just painted wall you need to cater to a crowd travelers who want to party shouldn t be mixed together with people who just like to hangout and site see that s my two cents at least take it with a grain of salt i ve never ran a hostel myself\"\n",
    "    , attencoder_model, attinf_model, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it s a good idea\n"
     ]
    }
   ],
   "source": [
    "print(translate(\n",
    "\"i typically play by getting rid of all of but my essentials and then trek around the wilderness for hours till i m overencumbered then drop the least valuable thing and start the trek back i only fast travel from settlements i ve built a zeppelin tower from and then it s still a long trek back that last part is more how i wished fast travel worked so i set my own limits and it does wonders for immersion also helps me discover more locations since i don t fast travel everywhere but on those long treks i could be out for hours and not have an auto save more my own fault than anything but i ve gotten stuck in those perpetually falling wedges of rock on said treks and had nobody but myself to blame for getting stuck\"\n",
    "    , attencoder_model, attinf_model, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffnum = []\n",
    "for i in range(1000):\n",
    "    example = Gen.tldr.aggregate([{'$sample':{'size': 1}}]).next()\n",
    "    gr_len = example['summary_len']\n",
    "    pr_len = len(translate(example['content'], attencoder_model, attinf_model, True).split())\n",
    "    diffnum.append(gr_len - pr_len)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 27, 138, 205, 484,   0]), array([  0,   1,   5,  10,  50, 100]))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(diffnum, bins=[0,1,5,10,50,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " what do we do\n"
     ]
    }
   ],
   "source": [
    "print(translate(\n",
    "    \"I finished Path of Daggers earlier this month and took a break to read I Am Pilgrim , I HIGHLY recommend , and I am now ready to start my journey into book 9 . One thing , I've forgotten a few plot threads . I can't search for them as possible spoilers , so what do I need to know going in ? Oh , and SPOILERS for those not up to here and please , no spoilers for me . I love this series to much for it to be ruined . I know Faile has been taken but I'm not sure about the other main characters where abouts and smaller character plots .\"\n",
    "        , attencoder_model, attinf_model, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other neat thing we can do with attention is investigate what the model is paying attention to in the encoder inputs when it is performing translations. We added this functionality to our LSTMAttentionCell class already, we just need to turn it on. \n",
    "\n",
    "Note that when this feature is turned on, the decoder no longer outputs word IDs. This means we'll need to revert back to using our teacher data to guide the decoder through an output phrase while we track where the model is paying attention.\n",
    "\n",
    "The below cell generates a table that is colored according to attention. Hotter, yellow colors correspond to higher attention, while darker blues correspond to less attention.\n",
    "\n",
    "If you bother to train the full data set, play around with this a bit. I feel compelled to point out how remarkable this learned behavior is. If you use this same function with untrained weights, the matrix below is randomly distributed with great uniformity. The training process creates all of the variety that you see, and simply because we constrained our model in a particular way. This emergence of meaning and overall semantic understanding from raw data is what makes machine learning so cool to me. It truly is a thing of beauty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_attention(input_sentence, output_sentence, infenc_model, infmodel):\n",
    "    sv = sentence_to_vector(input_sentence, input_lang)\n",
    "    # Shape=samples,sequence length\n",
    "    sv = sv.reshape(1,len(sv))\n",
    "    [emb_out, sh, sc] = infenc_model.predict(x=sv)\n",
    "    \n",
    "    outvec = sentence_to_vector(output_sentence, target_lang)\n",
    "    i = 0\n",
    "    cur_vec = np.zeros((1,1))\n",
    "    cur_vec[0,0] = outvec[0]\n",
    "    cur_word = \"<start>\"\n",
    "    output_attention = []\n",
    "    while i < (len(outvec)-1):\n",
    "        i += 1\n",
    "        x_in = [cur_vec, sh, sc, emb_out]\n",
    "        [nvec, sh, sc] = infmodel.predict(x=x_in)\n",
    "        output_attention += [nvec]\n",
    "        cur_vec[0,0] = outvec[i]\n",
    "    return output_attention\n",
    "\n",
    "def plotAttention(attMatrix):\n",
    "    attMatrix = np.asarray(attMatrix)\n",
    "    attMatrix = np.reshape(attMatrix, (attMatrix.shape[0], attMatrix.shape[-1]))\n",
    "    #print(attMatrix)\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attMatrix, aspect=\"auto\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "attencoder_model, attinf_model = createAttentionInference(True)\n",
    "#print(investigate_attention(\"I love me\", attencoder_model, attinf_model, True))\n",
    "#print(investigate_attention(\"I am hungry\", attencoder_model, attinf_model, True))\n",
    "plotAttention(investigate_attention(\"You can use a dictionary for this exam.\", \"Para este examen podéis usar un diccionario.\", attencoder_model, attinf_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
